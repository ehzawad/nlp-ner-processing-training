{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1386fe-dc34-40c7-84c3-8344125713d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "# import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "VISUALIZATION_STATUS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39961ff-505e-46f0-bddf-c28ffea499b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spacy_format(data, output_path=\"/home/ehz/nlp-process/data/unkonwn_ehza.spacy\"):\n",
    "    nlp = spacy.blank(\"bn\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "    c = 0\n",
    "    for text, annot in tqdm(data): # data in previous format\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "            \n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"strict\")\n",
    "            # print(start, end, label, span)\n",
    "            if span is None:\n",
    "                s = doc.text\n",
    "                sub_E = s[end:]\n",
    "                sub_S = s[:start]\n",
    "                end = end+ (0 if len(sub_E.split(\" \", 1)[0]) <= 0 else len(sub_E.split(\" \", 1)[0]))\n",
    "                start = start - (0 if len(sub_S.rsplit(\" \", 1)[-1]) <= 0 else len(sub_S.rsplit(\" \", 1)[-1]))\n",
    "                \n",
    "                span = doc.char_span(start, end, label=label, alignment_mode=\"strict\")\n",
    "                # print(\"Text:\", text)\n",
    "                # print(\"============:\", s[start:end], \"span:\", span, label)\n",
    "                if span is None:\n",
    "                    print(\"++++++++++++++++++++++++++++Skipping entity Start++++++++++++++++++++++++++++\")\n",
    "                    print(start, end, label, span)\n",
    "                    print(doc.text[start:end],doc.text[start],doc.text[end-1],'kh',sep='|')\n",
    "                    print(\"++++++++++++++++++++++++++++Skipping entity End++++++++++++++++++++++++++++++\")\n",
    "                    break\n",
    "                    c+=1\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents # label the text with the ents\n",
    "        if VISUALIZATION_STATUS:\n",
    "            spacy.displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "        db.add(doc)\n",
    "    db.to_disk(output_path) # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af58fdbe-86e6-4ecf-96aa-65e683398218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/ehz/.local/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: jinja2 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (23.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (1.25.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ehz/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ehz/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ehz/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ehz/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ehz/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ehz/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.1.1\n",
      "    Uninstalling pydantic-2.1.1:\n",
      "      Successfully uninstalled pydantic-2.1.1\n",
      "Successfully installed pydantic-1.10.12\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066bce36-d115-4f99-a906-4e64cc438802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doccano_to_spacy(doccano_JSON_FilePath):\n",
    "    # try:\n",
    "    training_data = []\n",
    "    lines=[]\n",
    "    with open(doccano_JSON_FilePath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        # print(data)\n",
    "        text = data['text']\n",
    "        entities = data['label']\n",
    "        if len(entities)>0:\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32d38ae-cf58-480d-b97c-fa615b8580ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for item in data:\n",
    "            json.dump(item, file, ensure_ascii=False)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad74fa9e-21e8-4619-b804-11d93c0a47d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of data : 3349\n",
      "Training Data: 3181\n",
      "Validation Data: 168\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(training_data))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(validation_data))\n\u001b[0;32m---> 21\u001b[0m \u001b[43msave_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m save_jsonl(training_data, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     23\u001b[0m convert_spacy_format(training_data, output_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.spacy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36msave_jsonl\u001b[0;34m(data, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_jsonl\u001b[39m(data, filename):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m      4\u001b[0m             json\u001b[38;5;241m.\u001b[39mdump(item, file, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train.jsonl'"
     ]
    }
   ],
   "source": [
    "data_file_path = \"/home/ehz/nlp-processing/outputfile.jsonl\"\n",
    "\n",
    "output_dir = \"/home/ehz/nlp-processing/data\"\n",
    "\n",
    "training_percentage = 95\n",
    "\n",
    "data = convert_doccano_to_spacy(data_file_path)\n",
    "random.shuffle(data)\n",
    "\n",
    "\n",
    "train_lenght = int((len(data)*training_percentage)/100)\n",
    "\n",
    "\n",
    "training_data = data[:train_lenght]\n",
    "validation_data = data[train_lenght:]\n",
    "\n",
    "print(\"Total Number of data :\", len(data))\n",
    "print(\"Training Data:\",len(training_data))\n",
    "print(\"Validation Data:\",len(validation_data))\n",
    "\n",
    "save_jsonl(training_data, os.path.join(output_dir, \"train.jsonl\"))\n",
    "save_jsonl(training_data, os.path.join(output_dir,\"val.jsonl\"))\n",
    "convert_spacy_format(training_data, output_path=os.path.join(output_dir,\"train.spacy\"))\n",
    "convert_spacy_format(validation_data, output_path=os.path.join(output_dir,\"val.spacy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00961952-2abc-4088-a502-fa28b3b6cbca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
